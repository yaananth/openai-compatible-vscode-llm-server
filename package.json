{
  "name": "openai-compatible-vscode-llm-server",
  "displayName": "OpenAI compatible VSCode LLM Server",
  "description": "OpenAI compatible VSCode LLM Server",
  "publisher": "vunguyen9584",
  "icon": "icon.png",
  "version": "0.0.7",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/username/openai-compatible-vscode-llm-server.git"
  },
  "engines": {
    "vscode": "^1.96.0"
  },
  "categories": [
    "Other",
    "Machine Learning",
    "AI"
  ],
  "keywords": [
    "openai",
    "llm",
    "api",
    "server",
    "language model"
  ],
  "activationEvents": [
    "onStartupFinished"
  ],
  "main": "./out/extension.js",
  "contributes": {
    "configuration": {
      "title": "OpenAI Compatible Server",
      "properties": {
        "openaiCompatibleServer.port": {
          "type": "number",
          "default": 3775,
          "description": "Port number for the OpenAI compatible server"
        },
        "openaiCompatibleServer.defaultModel": {
          "type": "string",
          "default": "anthropic/claude-3.7-sonnet:thinking",
          "enum": [
            "anthropic/claude-3.7-sonnet:thinking",
            "claude-sonnet-4.5",
            "claude-sonnet-4",
            "claude-opus-4.1",
            "claude-opus-4",
            "claude-3.7-sonnet-thought",
            "claude-3.7-sonnet",
            "claude-3.5-sonnet",
            "gpt-5",
            "gpt-5-mini",
            "gpt-5-codex",
            "gpt-4.1",
            "gpt-4o",
            "gpt-4o-mini",
            "gpt-4-turbo",
            "gpt-4",
            "gpt-3.5-turbo",
            "grok-code",
            "o4-mini",
            "o3",
            "o3-mini",
            "o1",
            "gemini-2.5-pro",
            "gemini-2.0-flash"
          ],
          "description": "Default model to use when none is specified"
        },
        "openaiCompatibleServer.autoStart": {
          "type": "boolean",
          "default": false,
          "description": "Automatically start the server on extension activation"
        }
      }
    },
    "commands": [
      {
        "command": "openai-compatible-vscode-llm-server.start",
        "title": "Start OpenAI Server",
        "category": "OpenAI Server"
      },
      {
        "command": "openai-compatible-vscode-llm-server.stop",
        "title": "Stop OpenAI Server",
        "category": "OpenAI Server"
      },
      {
        "command": "openai-compatible-vscode-llm-server.status",
        "title": "Show OpenAI Server Status",
        "category": "OpenAI Server"
      },
      {
        "command": "openai-compatible-vscode-llm-server.viewLogs",
        "title": "View Server Logs",
        "category": "OpenAI Compatible Server"
      }
    ]
  },
  "scripts": {
    "vscode:prepublish": "echo Skipping prepublish - using existing compiled code",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "package:vsix": "npx @vscode/vsce package --no-dependencies --out .vsix/openai-compatible-vscode-llm-server.vsix",
    "pretest": "npm run compile && npm run lint",
    "lint": "eslint src",
    "test": "vscode-test"
  },
  "devDependencies": {
    "@types/body-parser": "^1.19.6",
    "@types/mocha": "^10.0.10",
    "@types/node": "20.x",
    "@types/vscode": "^1.96.0",
    "@typescript-eslint/eslint-plugin": "^8.22.0",
    "@typescript-eslint/parser": "^8.22.0",
    "@vscode/test-cli": "^0.0.10",
    "@vscode/test-electron": "^2.4.1",
    "@vscode/vsce": "^3.1.1",
    "eslint": "^9.19.0",
    "typescript": "^5.9.3"
  },
  "dependencies": {
    "@types/express": "^5.0.0",
    "body-parser": "^2.2.0",
    "express": "^4.21.2"
  }
}
